{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "# Text classification: sentiment analysis \n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "## Popular tasks of text classification\n",
    "\n",
    "</font>\n",
    "\n",
    "- **Spam detection**: Having message decide is is spammy or not \n",
    "- **Topic identification**: Having article choose one of known classes like \"Sport\", \"Technology\", \"Finances\"\n",
    "- **Sentiment analysis**: Is the moview positive or negative \n",
    "- **Spelling correction**: what is more suitable \"weather\" or \"whether\"  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "## Features from Text\n",
    "\n",
    "</font>\n",
    "\n",
    "1. The most common words\n",
    "2. *Stop* words\n",
    "3. Normalization: lower case / stemming / lemmatizing\n",
    "4. Capitalization as feature \n",
    "5. POS e.g. \"the weather\" vs whether  \n",
    "6. grouping\n",
    "    - buy, purchase\n",
    "    - Mr, Ms, Dr\n",
    "    - Numbers\n",
    "    - Dates\n",
    "7. Bigrams, n-grams e.g. \"White House\"\n",
    "8. Sub-sequences e.g. \"ing\", \"ion\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "## Naive Bayes Classifiers\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Text classification of search query \n",
    "\n",
    "</font>\n",
    "\n",
    "- **python**  as snake -> Zoology\n",
    "- **python**  as programming language -> Computer Science\n",
    "- **python**  as \"monty python\" -> Entertainment\n",
    "\n",
    "Probabilistic model:\n",
    "\n",
    "#### Bayes Rule\n",
    "\n",
    "\\begin{equation*}\n",
    "P(y|X) = \\frac{P(X| y) \\cdot P(y)}{P(X)} \n",
    "\\quad\\quad\\quad\n",
    "Posterior = \\frac{ Likelihood \\cdot Prior}{Evidence} \n",
    "\\quad\\quad\\quad\n",
    "P(class| python) = \\frac{P(python| class) \\cdot P(class)}{P(python)} \n",
    "\\end{equation*}\n",
    "\n",
    "Considering the $P(python)$ is common for all classes we may compare just nominators: \n",
    "\n",
    "\\begin{equation*}\n",
    "P(python| Zoology) \\cdot P(Zoology) \n",
    "\\quad\\quad\\quad \n",
    "P(python|CS) \\cdot P(CS) \n",
    "\\quad\\quad\\quad\n",
    "P(python|Entertainment) \\cdot P(Entertainment) \n",
    "\\end{equation*}\n",
    "\n",
    "In general: \n",
    "\\begin{equation*}\n",
    "\\hat{y} =  \\underset{y}{argmax} \\quad P(y|X) =  \\underset{argmax}{y} P(X|y) \\cdot P(y)\n",
    "\\end{equation*}\n",
    "\n",
    "Most probably predicted class is <font color = blue>CS</font>\n",
    "\n",
    "#### Naive Bayes Classifiers\n",
    "\\begin{equation*}\n",
    "\\hat{y} =  \\underset{y}{argmax} \\quad P(y) \\prod_{ i=1 }^{ n }{ P(x_{ i }|\\,y) } \n",
    "\\end{equation*}\n",
    "\n",
    "If search query = **\"python snake\"** \n",
    "\\begin{equation*}\n",
    "\\hat{y} =  \\underset{y}{argmax} \\quad\n",
    "P(y)\\cdot P(python|\\,y) \\cdot P(snake|\\,y)\n",
    "\\end{equation*}\n",
    "\n",
    "Now, the most probably predicted class is <font color = blue>Zoology</font> since  $P(snake|\\,CS)$ is far less than $P(snake|\\,Zoology)$\n",
    "\n",
    "Note: if one of word is not presented in text then its statistical propability = 0 and as the result\n",
    "the whole likelihood = 0 regardless of other words. Thus it is worth using laplace smooting  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Laplace smooting \n",
    " \n",
    "</font>\n",
    "\n",
    "\n",
    "$\n",
    "A : 1 \\quad\n",
    "B : 3\\quad\n",
    "C : 0\\quad\n",
    "D : 6\\quad\n",
    "$\n",
    "\n",
    "$N= 10\\quad K =4$ \n",
    "<br>N - number of samples, K - number of classes\n",
    "\n",
    "\\begin{equation*}\n",
    "P(A) = 0.1\\quad\\quad\\quad\\quad\n",
    "P(B) = 0.3\\quad\\quad\\quad\\quad\n",
    "P(C) = 0.0\\quad\\quad\\quad\\quad\n",
    "P(D) = 0.6\\\\\n",
    "\\end{equation*}\n",
    "\n",
    "<font color = blue >\n",
    "\n",
    "\\begin{equation*}\n",
    "P^{\\,L}(x_{i}) =  \\frac{P(x_{i})+1}{N+K}\n",
    "\\end{equation*}\n",
    "\n",
    "</font>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "P^{\\,L}(A) =  \\frac{1+1}{10+4} = 0.14 \\quad P^{\\,L}(B) =  \\frac{3+1}{10+4} = 0.29\n",
    "\\quad P^{\\,L}(C) =  \\frac{0+1}{10+4} = 0.07 \\quad P^{\\,L}(D) =  \\frac{6+1}{10+4} = 0.5\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "## Sentiment Analysis\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Using NLTK\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews \n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /home/bohdan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('movie_reviews')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Load data\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "way because someone is apparently assuming that the genre is still hot with the kids . \n",
      "it also wrapped production two years ago and has been sitting on the shelves ever since . \n",
      "whatever . . . skip \n",
      "it ! \n",
      "where's joblo coming from ? \n",
      "a nightmare of elm street 3 ( 7/10 ) - blair witch 2 ( 7/10 ) - the crow ( 9/10 ) - the crow : salvation ( 4/10 ) - lost highway ( 10/10 ) - memento ( 10/10 ) - the others ( 9/10 ) - stir of echoes ( 8/10 ) \n",
      "the happy bastard's quick movie review \n",
      "damn that y2k bug . \n",
      "it's got a head start in this movie starring jamie lee curtis and another baldwin brother ( william this time ) in a story regarding a crew of a tugboat that comes across a deserted russian tech ship that has a strangeness to it when they kick the power back on . \n",
      "little do they know the power within . . . \n",
      "going for the gore and bringing on a few action sequences here and there , virus still feels very empty , like a movie going for all flash and no substance . \n",
      "we don't know why the crew w\n"
     ]
    }
   ],
   "source": [
    "all_movie_reviews_text= movie_reviews.raw() # it is just all reviews joined into one text e.g. \n",
    "# this is the ending of first review: \" the others ( 9/10 ) - stir of echoes ( 8/10 ) \"\n",
    "# this is the beginning of second review : \"the happy bastard's quick movie review \"\n",
    "print(all_movie_reviews_text[3600:4600])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Tools to review data \n",
    "\n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neg', 'pos']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cats =  movie_reviews.categories()\n",
    "cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neg/cv000_29416.txt',\n",
       " 'neg/cv001_19502.txt',\n",
       " 'neg/cv002_17424.txt',\n",
       " 'neg/cv003_12683.txt',\n",
       " 'neg/cv004_12641.txt',\n",
       " 'neg/cv005_29357.txt',\n",
       " 'neg/cv006_17022.txt',\n",
       " 'neg/cv007_4992.txt',\n",
       " 'neg/cv008_29326.txt',\n",
       " 'neg/cv009_29417.txt']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat = cats[0]\n",
    "ids= movie_reviews.fileids(cat)\n",
    "ids[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plot : two teen couples go to a church party , drink and then drive . \n",
      "they get into an accident . \n",
      "one of the guys dies , but his girlfriend continues to see him in her life , and has nightmares . \n",
      "what's the deal ? \n",
      "watch the movie and \" sorta \" find out . . . \n",
      "critique : a mind-fuck movie for the teen generation that touches on a very cool idea , but presents it in a very bad package . \n",
      "which is what makes this review an even harder one to write , since i generally applaud films which attempt to break the mold , mess with your head and such ( lost highway & memento ) , but there are good and bad ways of making all types of films , and these folks just didn't snag this one correctly . \n",
      "they seem to have taken this pretty neat concept , but executed it terribly . \n",
      "so what are the problems with the movie ? \n",
      "well , its main problem is that it's simply too jumbled . \n",
      "it starts off \" normal \" but then downshifts into this \" fantasy \" world in which you , as an audience member , have no idea what's going on . \n",
      "there are dreams , there are characters coming back from the dead , there are others who look like the dead , there are strange apparitions , there are disappearances , there are a looooot of chase scenes , there are tons of weird things that happen , and most of it is simply not explained . \n",
      "now i personally don't mind trying to unravel a film every now and then , but when all it does is give me the same clue over and over again , i get kind of fed up after a while , which is this film's biggest problem . \n",
      "it's obviously got this big secret to hide , but it seems to want to hide it completely until its final five minutes . \n",
      "and do they make things entertaining , thrilling or even engaging , in the meantime ? \n",
      "not really . \n",
      "the sad part is that the arrow and i both dig on flicks like this , so we actually figured most of it out by the half-way point , so all of the strangeness after that did start to make a little bit of sense , but it still didn't the make the film all that more entertaining . \n",
      "i guess the bottom line with movies like this is that you should always make sure that the audience is \" into it \" even before they are given the secret password to enter your world of understanding . \n",
      "i mean , showing melissa sagemiller running away from visions for about 20 minutes throughout the movie is just plain lazy ! ! \n",
      "okay , we get it . . . there \n",
      "are people chasing her and we don't know who they are . \n",
      "do we really need to see it over and over again ? \n",
      "how about giving us different scenes offering further insight into all of the strangeness going down in the movie ? \n",
      "apparently , the studio took this film away from its director and chopped it up themselves , and it shows . \n",
      "there might've been a pretty decent teen mind-fuck movie in here somewhere , but i guess \" the suits \" decided that turning it into a music video with little edge , would make more sense . \n",
      "the actors are pretty good for the most part , although wes bentley just seemed to be playing the exact same character that he did in american beauty , only in a new neighborhood . \n",
      "but my biggest kudos go out to sagemiller , who holds her own throughout the entire film , and actually has you feeling her character's unraveling . \n",
      "overall , the film doesn't stick because it doesn't entertain , it's confusing , it rarely excites and it feels pretty redundant for most of its runtime , despite a pretty cool ending and explanation to all of the craziness that came before it . \n",
      "oh , and by the way , this is not a horror or teen slasher flick . . . it's \n",
      "just packaged to look that way because someone is apparently assuming that the genre is still hot with the kids . \n",
      "it also wrapped production two years ago and has been sitting on the shelves ever since . \n",
      "whatever . . . skip \n",
      "it ! \n",
      "where's joblo coming from ? \n",
      "a nightmare of elm street 3 ( 7/10 ) - blair witch 2 ( 7/10 ) - the crow ( 9/10 ) - the crow : salvation ( 4/10 ) - lost highway ( 10/10 ) - memento ( 10/10 ) - the others ( 9/10 ) - stir of echoes ( 8/10 ) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "id_review = ids[0]\n",
    "print(movie_reviews.raw(id_review))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Tokenize\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1336782\n",
      "['plot', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', 'drink', 'and', 'then', 'drive', 'they', 'get', 'into', 'an', 'accident', 'one', 'of', 'the', 'guys', 'dies', 'but', 'his', 'girlfriend', 'continues', 'to', 'see', 'him', 'in', 'her', 'life', 'and', 'has', 'nightmares', 'what', 's', 'the', 'deal', 'watch', 'the', 'movie', 'and', 'sorta', 'find', 'out', 'critique', 'a', 'mind', 'fuck', 'movie', 'for', 'the', 'teen', 'generation', 'that', 'touches', 'on', 'a', 'very', 'cool', 'idea', 'but', 'presents', 'it', 'in', 'a', 'very', 'bad', 'package', 'which', 'is', 'what', 'makes', 'this', 'review', 'an', 'even', 'harder', 'one', 'to', 'write', 'since', 'i', 'generally', 'applaud', 'films', 'which', 'attempt', 'to', 'break', 'the', 'mold', 'mess', 'with', 'your', 'head', 'and', 'such']\n"
     ]
    }
   ],
   "source": [
    "def preprocess(text): # removes punctualtion\n",
    "    tokenizer = RegexpTokenizer(r'\\w+') # just for demo \n",
    "    return tokenizer.tokenize(text.lower())\n",
    "\n",
    "all_words = preprocess(all_movie_reviews_text)\n",
    "print (len(all_words))\n",
    "print(all_words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Build vocabulary\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of vocabulary: 39,696\n",
      "('the', 'a', 'and', 'of', 'to', 'is', 'in', 's', 'it', 'that', 'as', 'with', 'for', 'his', 'this', 'film', 'i', 'he', 'but', 'on', 'are', 't', 'by', 'be', 'one', 'movie', 'an', 'who', 'not', 'you', 'from', 'at', 'was', 'have', 'they', 'has', 'her', 'all', 'there', 'like', 'so', 'out', 'about', 'up', 'more', 'what', 'when', 'which', 'or', 'she', 'their', 'some', 'just', 'can', 'if', 'we', 'him', 'into', 'even', 'only', 'than', 'no', 'good', 'time', 'most', 'its', 'will', 'story', 'would', 'been', 'much', 'character', 'also', 'get', 'other', 'do', 'two', 'well', 'them', 'very', 'characters', 'first', 'after', 'see', 'way', 'because', 'make', 'life', 'off', 'too', 'any', 'does', 'really', 'had', 'while', 'films', 'how', 'plot', 'little', 'where')\n"
     ]
    }
   ],
   "source": [
    "all_words=nltk.FreqDist(all_words)\n",
    "print ('len of vocabulary: {:,}'.format (len(all_words)))\n",
    "# Use most common words\n",
    "most_common_words = list(zip(*all_words.most_common()))[0] # [0] means names whereas [1] are frequencies \n",
    "# most_common(5000) - it may retutn limited number but in this sample the features will be filtered later after removing stop words \n",
    "print (most_common_words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Get rid of stop words \n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(words):\n",
    "    stop_words = set(stopwords.words('english'))  \n",
    "    return [w for w in words if w not in stop_words]\n",
    "most_common_words_filtered = remove_stop_words(most_common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Select features \n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['film', 'one', 'movie', 'like', 'even', 'good', 'time', 'story', 'would', 'much', 'character', 'also', 'get', 'two', 'well', 'characters', 'first', 'see', 'way', 'make', 'life', 'really', 'films', 'plot', 'little', 'people', 'could', 'scene', 'man', 'bad', 'never', 'best', 'new', 'scenes', 'many', 'director', 'know', 'movies', 'action', 'great', 'another', 'love', 'go', 'made', 'us', 'big', 'end', 'something', 'back', 'still', 'world', 'seems', 'work', 'makes', 'however', 'every', 'though', 'better', 'real', 'audience', 'enough', 'seen', 'take', 'around', 'going', 'year', 'performance', 'role', 'old', 'gets', 'may', 'things', 'think', 'years', 'last', 'comedy', 'funny', 'actually', 'long', 'look', 'almost', 'thing', 'fact', 'nothing', 'say', 'right', 'john', 'although', 'played', 'find', 'script', 'come', 'ever', 'cast', 'since', 'star', 'plays', 'young', 'show', 'comes']\n"
     ]
    }
   ],
   "source": [
    "word_features = most_common_words_filtered [:3000]\n",
    "print (word_features[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Extract documents and labels\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: this does not use tokenizing to documents but words of document retrieved by file_id instead.\n",
    "documents = [(list(movie_reviews.words(file_id)), category) # using the words() method of movie_reviews object\n",
    "             for category in movie_reviews.categories() # select category - there are two: ['neg', 'pos']\n",
    "             for file_id in movie_reviews.fileids(category)]# select all file_ids for specified category\n",
    "len (documents)\n",
    "# This returns list of tuples (list_of_tokens_of document, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive', '.', 'they', 'get', 'into', 'an', 'accident', '.', 'one', 'of', 'the', 'guys', 'dies', ',', 'but', 'his', 'girlfriend', 'continues', 'to', 'see', 'him', 'in', 'her', 'life', ',', 'and', 'has', 'nightmares', '.', 'what', \"'\", 's', 'the', 'deal', '?', 'watch', 'the', 'movie', 'and', '\"', 'sorta', '\"', 'find', 'out', '.', '.', '.', 'critique', ':', 'a', 'mind', '-', 'fuck', 'movie', 'for', 'the', 'teen', 'generation', 'that', 'touches', 'on', 'a', 'very', 'cool', 'idea', ',', 'but', 'presents', 'it', 'in', 'a', 'very', 'bad', 'package', '.', 'which', 'is', 'what', 'makes', 'this', 'review', 'an', 'even', 'harder', 'one', 'to', 'write', ',', 'since', 'i', 'generally', 'applaud', 'films', 'which', 'attempt', 'to', 'break', 'the', 'mold', ',', 'mess', 'with', 'your', 'head', 'and', 'such', '(', 'lost', 'highway', '&', 'memento', ')', ',', 'but', 'there', 'are', 'good', 'and', 'bad', 'ways', 'of', 'making', 'all', 'types', 'of', 'films', ',', 'and', 'these', 'folks', 'just', 'didn', \"'\", 't', 'snag', 'this', 'one', 'correctly', '.', 'they', 'seem', 'to', 'have', 'taken', 'this', 'pretty', 'neat', 'concept', ',', 'but', 'executed', 'it', 'terribly', '.', 'so', 'what', 'are', 'the', 'problems', 'with', 'the', 'movie', '?', 'well', ',', 'its', 'main', 'problem', 'is', 'that', 'it', \"'\", 's', 'simply', 'too', 'jumbled', '.', 'it', 'starts', 'off', '\"', 'normal', '\"', 'but', 'then', 'downshifts', 'into', 'this', '\"', 'fantasy', '\"', 'world', 'in', 'which', 'you', ',', 'as', 'an', 'audience', 'member', ',', 'have', 'no', 'idea', 'what', \"'\", 's', 'going', 'on', '.', 'there', 'are', 'dreams', ',', 'there', 'are', 'characters', 'coming', 'back', 'from', 'the', 'dead', ',', 'there', 'are', 'others', 'who', 'look', 'like', 'the', 'dead', ',', 'there', 'are', 'strange', 'apparitions', ',', 'there', 'are', 'disappearances', ',', 'there', 'are', 'a', 'looooot', 'of', 'chase', 'scenes', ',', 'there', 'are', 'tons', 'of', 'weird', 'things', 'that', 'happen', ',', 'and', 'most', 'of', 'it', 'is', 'simply', 'not', 'explained', '.', 'now', 'i', 'personally', 'don', \"'\", 't', 'mind', 'trying', 'to', 'unravel', 'a', 'film', 'every', 'now', 'and', 'then', ',', 'but', 'when', 'all', 'it', 'does', 'is', 'give', 'me', 'the', 'same', 'clue', 'over', 'and', 'over', 'again', ',', 'i', 'get', 'kind', 'of', 'fed', 'up', 'after', 'a', 'while', ',', 'which', 'is', 'this', 'film', \"'\", 's', 'biggest', 'problem', '.', 'it', \"'\", 's', 'obviously', 'got', 'this', 'big', 'secret', 'to', 'hide', ',', 'but', 'it', 'seems', 'to', 'want', 'to', 'hide', 'it', 'completely', 'until', 'its', 'final', 'five', 'minutes', '.', 'and', 'do', 'they', 'make', 'things', 'entertaining', ',', 'thrilling', 'or', 'even', 'engaging', ',', 'in', 'the', 'meantime', '?', 'not', 'really', '.', 'the', 'sad', 'part', 'is', 'that', 'the', 'arrow', 'and', 'i', 'both', 'dig', 'on', 'flicks', 'like', 'this', ',', 'so', 'we', 'actually', 'figured', 'most', 'of', 'it', 'out', 'by', 'the', 'half', '-', 'way', 'point', ',', 'so', 'all', 'of', 'the', 'strangeness', 'after', 'that', 'did', 'start', 'to', 'make', 'a', 'little', 'bit', 'of', 'sense', ',', 'but', 'it', 'still', 'didn', \"'\", 't', 'the', 'make', 'the', 'film', 'all', 'that', 'more', 'entertaining', '.', 'i', 'guess', 'the', 'bottom', 'line', 'with', 'movies', 'like', 'this', 'is', 'that', 'you', 'should', 'always', 'make', 'sure', 'that', 'the', 'audience', 'is', '\"', 'into', 'it', '\"', 'even', 'before', 'they', 'are', 'given', 'the', 'secret', 'password', 'to', 'enter', 'your', 'world', 'of', 'understanding', '.', 'i', 'mean', ',', 'showing', 'melissa', 'sagemiller', 'running', 'away', 'from', 'visions', 'for', 'about', '20', 'minutes', 'throughout', 'the', 'movie', 'is', 'just', 'plain', 'lazy', '!', '!', 'okay', ',', 'we', 'get', 'it', '.', '.', '.', 'there', 'are', 'people', 'chasing', 'her', 'and', 'we', 'don', \"'\", 't', 'know', 'who', 'they', 'are', '.', 'do', 'we', 'really', 'need', 'to', 'see', 'it', 'over', 'and', 'over', 'again', '?', 'how', 'about', 'giving', 'us', 'different', 'scenes', 'offering', 'further', 'insight', 'into', 'all', 'of', 'the', 'strangeness', 'going', 'down', 'in', 'the', 'movie', '?', 'apparently', ',', 'the', 'studio', 'took', 'this', 'film', 'away', 'from', 'its', 'director', 'and', 'chopped', 'it', 'up', 'themselves', ',', 'and', 'it', 'shows', '.', 'there', 'might', \"'\", 've', 'been', 'a', 'pretty', 'decent', 'teen', 'mind', '-', 'fuck', 'movie', 'in', 'here', 'somewhere', ',', 'but', 'i', 'guess', '\"', 'the', 'suits', '\"', 'decided', 'that', 'turning', 'it', 'into', 'a', 'music', 'video', 'with', 'little', 'edge', ',', 'would', 'make', 'more', 'sense', '.', 'the', 'actors', 'are', 'pretty', 'good', 'for', 'the', 'most', 'part', ',', 'although', 'wes', 'bentley', 'just', 'seemed', 'to', 'be', 'playing', 'the', 'exact', 'same', 'character', 'that', 'he', 'did', 'in', 'american', 'beauty', ',', 'only', 'in', 'a', 'new', 'neighborhood', '.', 'but', 'my', 'biggest', 'kudos', 'go', 'out', 'to', 'sagemiller', ',', 'who', 'holds', 'her', 'own', 'throughout', 'the', 'entire', 'film', ',', 'and', 'actually', 'has', 'you', 'feeling', 'her', 'character', \"'\", 's', 'unraveling', '.', 'overall', ',', 'the', 'film', 'doesn', \"'\", 't', 'stick', 'because', 'it', 'doesn', \"'\", 't', 'entertain', ',', 'it', \"'\", 's', 'confusing', ',', 'it', 'rarely', 'excites', 'and', 'it', 'feels', 'pretty', 'redundant', 'for', 'most', 'of', 'its', 'runtime', ',', 'despite', 'a', 'pretty', 'cool', 'ending', 'and', 'explanation', 'to', 'all', 'of', 'the', 'craziness', 'that', 'came', 'before', 'it', '.', 'oh', ',', 'and', 'by', 'the', 'way', ',', 'this', 'is', 'not', 'a', 'horror', 'or', 'teen', 'slasher', 'flick', '.', '.', '.', 'it', \"'\", 's', 'just', 'packaged', 'to', 'look', 'that', 'way', 'because', 'someone', 'is', 'apparently', 'assuming', 'that', 'the', 'genre', 'is', 'still', 'hot', 'with', 'the', 'kids', '.', 'it', 'also', 'wrapped', 'production', 'two', 'years', 'ago', 'and', 'has', 'been', 'sitting', 'on', 'the', 'shelves', 'ever', 'since', '.', 'whatever', '.', '.', '.', 'skip', 'it', '!', 'where', \"'\", 's', 'joblo', 'coming', 'from', '?', 'a', 'nightmare', 'of', 'elm', 'street', '3', '(', '7', '/', '10', ')', '-', 'blair', 'witch', '2', '(', '7', '/', '10', ')', '-', 'the', 'crow', '(', '9', '/', '10', ')', '-', 'the', 'crow', ':', 'salvation', '(', '4', '/', '10', ')', '-', 'lost', 'highway', '(', '10', '/', '10', ')', '-', 'memento', '(', '10', '/', '10', ')', '-', 'the', 'others', '(', '9', '/', '10', ')', '-', 'stir', 'of', 'echoes', '(', '8', '/', '10', ')'], 'neg')\n"
     ]
    }
   ],
   "source": [
    "print (documents [0]) # (['plot', ':', 'two', 'teen', ... 'echoes', '(', '8', '/', '10', ')'], 'neg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Shuffle documents \n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shuffle first \n",
    "random.shuffle(documents) # it is inplace method\n",
    "documents= documents[:500] # reduce the data set for speed up the demo \n",
    "len (documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Vectorize documents \n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_features(review_tokens):\n",
    "    return {w: w in set(review_tokens) for w in word_features} # feature representation on document\n",
    "\n",
    "data_set= [(find_features(review_tokens), category) for (review_tokens, category) in documents]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_set[0]\n",
    "\n",
    "# ({'film': True,  <- exmp\n",
    "#   'one': True,\n",
    "#   'movie': True,\n",
    "#   'like': True,\n",
    "#   'even': False,\n",
    "#   'good': True,\n",
    "#   'time': False,\n",
    "#   'story': False,\n",
    "#   'would': True,\n",
    "#   'much': True,\n",
    "#   'character': True,\n",
    "#   'also': False,\n",
    "#   'get': True,\n",
    "#   'two': True,\n",
    "#   'well': True,\n",
    "#   'characters': False,\n",
    "#   'first': False,\n",
    "#   ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Split to training and test set\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n"
     ]
    }
   ],
   "source": [
    "split_on = int(len(data_set)*.8)\n",
    "X_y_train= data_set[:split_on]\n",
    "X_y_test = data_set[split_on:]\n",
    "print (len(X_y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Train model\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf= nltk.NaiveBayesClassifier.train(X_y_train) # Note: the difference grammar comparing with sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Evaluate model\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(clf, X_y_test)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Review most informative features\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                   awful = True              neg : pos    =      9.0 : 1.0\n",
      "             outstanding = True              pos : neg    =      9.0 : 1.0\n",
      "               realistic = True              pos : neg    =      9.0 : 1.0\n",
      "                  murphy = True              pos : neg    =      7.7 : 1.0\n",
      "              performers = True              neg : pos    =      7.7 : 1.0\n",
      "              remembered = True              pos : neg    =      7.7 : 1.0\n",
      "                   study = True              pos : neg    =      7.7 : 1.0\n",
      "                    dull = True              neg : pos    =      7.4 : 1.0\n",
      "                costumes = True              pos : neg    =      7.0 : 1.0\n",
      "               endearing = True              pos : neg    =      7.0 : 1.0\n",
      "                     era = True              pos : neg    =      7.0 : 1.0\n",
      "               animation = True              pos : neg    =      6.6 : 1.0\n",
      "               americans = True              pos : neg    =      6.3 : 1.0\n",
      "             brilliantly = True              pos : neg    =      6.3 : 1.0\n",
      "                designed = True              neg : pos    =      6.3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "clf.show_most_informative_features(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Incorporate with sklearn\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify.scikitlearn import SklearnClassifier # this is wrapper to incorporate with sklearn using nltk style.\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Convert to nltk classifiers \n",
    "MNNB_classifier= SklearnClassifier(MultinomialNB()) # Note : use ()\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr_classifier = SklearnClassifier(LogisticRegression()) \n",
    "\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC # NuSVC - Similar to SVC but uses a parameter to control the number of support vectors.\n",
    "svc_clf = SklearnClassifier(SVC())  \n",
    "lin_svc_clf= SklearnClassifier(LinearSVC())  \n",
    "nu_svc_clf = SklearnClassifier(NuSVC())  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy nltk.NaiveBayesClassifier=79.0%\n",
      "Accuracy MNNB_classifier =81.0%\n",
      "Accuracy lr_classifier =75.0%\n",
      "Accuracy svc_clf=73.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bohdan/anaconda3/envs/venv/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy lin_svc_clf=72.0%\n",
      "Accuracy nu_svc_clf=75.0%\n"
     ]
    }
   ],
   "source": [
    "# native nltk classifier\n",
    "clf= nltk.NaiveBayesClassifier.train(X_y_train) \n",
    "\n",
    "print('Accuracy nltk.NaiveBayesClassifier={}%'.format(nltk.classify.accuracy(clf,X_y_test) * 100))\n",
    "# clf.show_most_informative_features(15)\n",
    "\n",
    "MNNB_classifier.train(X_y_train)\n",
    "print('Accuracy MNNB_classifier ={}%'.format(nltk.classify.accuracy(MNNB_classifier, X_y_test) * 100)) # 79.0%\n",
    "\n",
    "lr_classifier.train(X_y_train)\n",
    "print('Accuracy lr_classifier ={}%'.format(nltk.classify.accuracy(lr_classifier, X_y_test) * 100)) # 82.0%\n",
    "\n",
    "svc_clf.train(X_y_train)\n",
    "print('Accuracy svc_clf={}%'.format(nltk.classify.accuracy(svc_clf, X_y_test) * 100)) # 52.0% - default is rbf kernel\n",
    "\n",
    "lin_svc_clf.train(X_y_train)\n",
    "print('Accuracy lin_svc_clf={}%'.format(nltk.classify.accuracy(lin_svc_clf, X_y_test) * 100)) # 82.0%\n",
    "\n",
    "nu_svc_clf.train(X_y_train)\n",
    "print('Accuracy nu_svc_clf={}%'.format(nltk.classify.accuracy(nu_svc_clf, X_y_test) * 100)) #\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Combining algos with a vote\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify import ClassifierI\n",
    "class Vote_Classifier(ClassifierI): # inherit\n",
    "    def __init__(self, *classifiers): # expecting list of classifiers\n",
    "        self.classifiers = classifiers\n",
    "\n",
    "    def classify(self, sample): \n",
    "        return mode([clf.classify(sample) for clf in self.classifiers]) \n",
    "\n",
    "    def calc_confidence(self, sample):\n",
    "        votes= [clf.classify(sample) for clf in self.classifiers] #\n",
    "        return votes.count(mode(votes))/len(votes) # fraction of how many votes match to mode to total votes number\n",
    "\n",
    "def mode(array): # returns first mode in case of multi modes\n",
    "    return max(set(array), key=array.count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a= [3,3,1,2,2]\n",
    "mode (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.count (mode(a))/len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy vote_clf=74.00%\n"
     ]
    }
   ],
   "source": [
    "vote_clf= Vote_Classifier(clf, lr_classifier, svc_clf, lin_svc_clf, nu_svc_clf)\n",
    "print('Accuracy vote_clf={:.2%}'.format(nltk.classify.accuracy(vote_clf, X_y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Classify new sample\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://timesofindia.indiatimes.com/entertainment/english/movie-reviews/cold-pursuit/movie-review/67892834.cms\n",
    "new_review = '''This unusual satire on gangsters and revenge stories, starts off with a quote from Oscar Wilde and some delightful background music that sets the tone for rest of the film. The first few minutes play out like a predictable thriller, featuring a wronged father and his pursuit for vigilante justice. But, what follows is a series of stylised killing sequences, that almost seem like parodies of action set pieces that you’ve seen Liam Neeson pulling off with deadpan ease in the past. Yet, director Hans Petter Noland, who also made the Norwegian film In Order Of Disappearance that inspired Cold Pursuit, and writer Frank Baldwin create a refreshing narrative full of memorable moments. The movie seems bizarrely funny and the snow-heavy setting creates the right atmosphere for the dry and cold-cut humour.\n",
    "The story begins with tragedy and the first few minutes seem dead serious, right up to the point where Coxman confronts his first victim, the gangster named Speedo. But, as the revenge-seeking father moves up the ranks of the mafia chain, the characters become quirky and the situations get thoroughly entertaining. The introduction of characters like Viking (Tom Bateman), the main antagonist and his team of crazy henchmen like Mustang, Dexter and more, alleviates the narrative. There’s also a track of warring mafia gangs as Viking wages a war against the native Indians led by White Bull (Tom Jackson). Cold Pursuit may not be too creative with the kill sequences, but it does get interesting with the wry sense of humour.\n",
    "Neeson does what he does best. He keeps a straight face and plays the game of intimidation with ease. He’s just a regular guy who’s way out of his league, killing gangsters. But, his outrageous mission is what makes the story interesting. Watch out for a superb cameo by William Forsythe, too, who plays a brief but key role in Coxman’s revenge saga.\n",
    "The way Cold Pursuit manages to blend sardonic humour with cold-blooded killings makes it reminiscent of movies like The Coen Brothers’ Fargo and Guy Ritchie’s Snatch. This one’s a refreshingly cool black-comedy that does wonders for the genre.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new  = find_features(preprocess(new_review))\n",
    "# x_new\n",
    "\n",
    "# {'film': True, <- exmp\n",
    "#  'one': True,\n",
    "#  'movie': True,\n",
    "#  'like': True,\n",
    "#  'even': False,\n",
    "#  'good': False,\n",
    "#  'time': False,\n",
    "#  'story': True,\n",
    "#  'would': False,\n",
    "#  'much': False,\n",
    "#  'character': False,\n",
    "#  'also': True,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification: pos\n",
      "Confidence: 100.00%\n"
     ]
    }
   ],
   "source": [
    "print('\\nClassification: {}\\nConfidence: {:.2%}'.format(\n",
    "    vote_clf.classify(x_new),vote_clf.calc_confidence(x_new)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "# Using sklearn\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Load data \n",
    "\n",
    "data set ['amazon-reviews-unlocked-mobile-phones'](https://www.kaggle.com/PromptCloudHQ/amazon-reviews-unlocked-mobile-phones)\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len=  413,840\n",
      "columns= ['Product Name', 'Brand Name', 'Price', 'Rating', 'Reviews', 'Review Votes']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Brand Name</th>\n",
       "      <th>Price</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Review Votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>5</td>\n",
       "      <td>I feel so LUCKY to have found this used (phone...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>4</td>\n",
       "      <td>nice phone, nice up grade from my pantach revu...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>5</td>\n",
       "      <td>Very pleased</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>4</td>\n",
       "      <td>It works good but it goes slow sometimes but i...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>4</td>\n",
       "      <td>Great phone to replace my lost phone. The only...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Product Name Brand Name   Price  \\\n",
       "0  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "1  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "2  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "3  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "4  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "\n",
       "   Rating                                            Reviews  Review Votes  \n",
       "0       5  I feel so LUCKY to have found this used (phone...           1.0  \n",
       "1       4  nice phone, nice up grade from my pantach revu...           0.0  \n",
       "2       5                                       Very pleased           0.0  \n",
       "3       4  It works good but it goes slow sometimes but i...           0.0  \n",
       "4       4  Great phone to replace my lost phone. The only...           0.0  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "cwd= os.getcwd() # current working directory\n",
    "# path = os.path.join(cwd,'data')\n",
    "fn=  'Amazon_Unlocked_Mobile.csv' # https://www.kaggle.com/PromptCloudHQ/amazon-reviews-unlocked-mobile-phones\n",
    "df = pd.read_csv(fn) # \n",
    "print('len=  {:,}\\ncolumns= {}'.format(len(df), list(df)))\n",
    "\n",
    "# df = df.sample(frac=0.1, random_state=10) # reduce the amount of reviews due to speedup the training considering this is demo\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Get rid of records with missed data \n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len=  334,335\n"
     ]
    }
   ],
   "source": [
    "df.dropna(inplace=True) \n",
    "print('len=  {:,}'.format(len(df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Label positive and negative \n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Brand Name</th>\n",
       "      <th>Price</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Review Votes</th>\n",
       "      <th>Rating_binary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>5</td>\n",
       "      <td>I feel so LUCKY to have found this used (phone...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>4</td>\n",
       "      <td>nice phone, nice up grade from my pantach revu...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>5</td>\n",
       "      <td>Very pleased</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>4</td>\n",
       "      <td>It works good but it goes slow sometimes but i...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>4</td>\n",
       "      <td>Great phone to replace my lost phone. The only...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Product Name Brand Name   Price  \\\n",
       "0  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "1  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "2  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "3  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "4  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "\n",
       "   Rating                                            Reviews  Review Votes  \\\n",
       "0       5  I feel so LUCKY to have found this used (phone...           1.0   \n",
       "1       4  nice phone, nice up grade from my pantach revu...           0.0   \n",
       "2       5                                       Very pleased           0.0   \n",
       "3       4  It works good but it goes slow sometimes but i...           0.0   \n",
       "4       4  Great phone to replace my lost phone. The only...           0.0   \n",
       "\n",
       "   Rating_binary  \n",
       "0              1  \n",
       "1              1  \n",
       "2              1  \n",
       "3              1  \n",
       "4              1  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df['Rating'] != 3] # Remove any 'neutral' ratings equal to 3  as uninformative\n",
    "df['Rating_binary'] = np.where(df['Rating'] > 3, 1, 0) # returns 1 for 4,5 and 0 for 1,2\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7482686025879323"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Rating_binary'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Split to train and test sets\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Reviews'],df['Rating_binary'],random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Review training sample\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('I bought a BB Black and was deliveried a White BB.Really is not a serious provider...Next time is better to cancel the order.',\n",
       " 0)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.iloc[0], y_train.iloc[0] # Be careful with quering like X_train[0] because it casts to X_train.loc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Extract Features \n",
    "\n",
    "</font>\n",
    "The bag-of-words approach is simple way to represent text for use in machine learning, which ignores structure and only counts how often each word occurs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Count vectorizer\n",
    "\n",
    "</font>\n",
    "By default, selects tokens of 2 or more alphanumeric characters (punctuation is completely ignored and always treated as a token separator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features samples:\n",
      "['00', '4less', 'adr6275', 'assignment', 'blazingly', 'cassettes', 'condishion', 'debi', 'dollarsshipping', 'esteem', 'flashy', 'gorila', 'human', 'irullu', 'like', 'microsaudered', 'nightmarish', 'p770', 'poori', 'quirky', 'responseive', 'send', 'sos', 'synch', 'trace', 'utiles', 'withstanding']\n",
      "\n",
      "len of features 53,216\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "vect = CountVectorizer().fit(X_train) # Fit the CountVectorizer to the training data\n",
    "print('features samples:\\n{}'.format(vect.get_feature_names()[::2000])) # display each 2000-th feature \n",
    "print ('\\nlen of features {:,}'.format(len(vect.get_feature_names()))) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Transfrom the X_train to feature representation\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<231207x53216 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 6117776 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vectorized = vect.transform(X_train) # indeces of existing words from vocabulary and their count in current text\n",
    "X_train_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4986)\t1\n",
      "  (0, 7259)\t2\n",
      "  (0, 7676)\t1\n",
      "  (0, 7878)\t1\n",
      "  (0, 8476)\t1\n",
      "  (0, 9637)\t1\n",
      "  (0, 14420)\t1\n",
      "  (0, 26003)\t2\n",
      "  (0, 31892)\t1\n",
      "  (0, 32284)\t1\n",
      "  (0, 33437)\t1\n",
      "  (0, 37356)\t1\n",
      "  (0, 38473)\t1\n",
      "  (0, 42146)\t1\n",
      "  (0, 46946)\t1\n",
      "  (0, 47462)\t1\n",
      "  (0, 47639)\t1\n",
      "  (0, 51169)\t1\n",
      "  (0, 51673)\t1\n"
     ]
    }
   ],
   "source": [
    "print (X_train_vectorized[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Review vectorized training sample\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53211</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53212</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53213</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53214</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53215</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>53216 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       value\n",
       "0          0\n",
       "1          0\n",
       "2          0\n",
       "3          0\n",
       "4          0\n",
       "...      ...\n",
       "53211      0\n",
       "53212      0\n",
       "53213      0\n",
       "53214      0\n",
       "53215      0\n",
       "\n",
       "[53216 rows x 1 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# review first sample \n",
    "df = pd.DataFrame(X_train_vectorized[0].toarray(), index= ['value']).T\n",
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4986, 7259, 7676, 7878, 8476, 9637, 14420, 26003, 31892, 32284, 33437, 37356, 38473, 42146, 46946, 47462, 47639, 51169, 51673]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['and',\n",
       " 'bb',\n",
       " 'better',\n",
       " 'black',\n",
       " 'bought',\n",
       " 'cancel',\n",
       " 'deliveried',\n",
       " 'is',\n",
       " 'next',\n",
       " 'not',\n",
       " 'order',\n",
       " 'provider',\n",
       " 'really',\n",
       " 'serious',\n",
       " 'the',\n",
       " 'time',\n",
       " 'to',\n",
       " 'was',\n",
       " 'white']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (list(df[df['value']>0].index))\n",
    "[vect.get_feature_names()[index] for index in df[df['value']>0].index.values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Train model\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(max_iter=2000).fit(X_train_vectorized, y_train) # Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Evaluate model\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1:  0.9691796161664887\n",
      "AUC:  0.9795194643755873\n"
     ]
    }
   ],
   "source": [
    "predictions = clf.predict(vect.transform(X_test)) # Predict the transformed test documents\n",
    "print('f1: ', f1_score(y_test, predictions)) \n",
    "scores = clf.decision_function(vect.transform(X_test)) \n",
    "print('AUC: ', roc_auc_score(y_test, scores)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Review relevant features \n",
    "    \n",
    "</font>\n",
    "\n",
    "The smallest coefs corresponds to `Neg` impact, and largest coefs represent `Pos` impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 53216),\n",
       " (53216,),\n",
       " [-4.672214571635938,\n",
       "  -3.64055966414183,\n",
       "  -3.618983577847033,\n",
       "  -3.522666860624433,\n",
       "  -3.3596758886687477,\n",
       "  -3.22971400669994,\n",
       "  -3.1379147375995147,\n",
       "  -3.1312129025960784,\n",
       "  -3.1309403193093246,\n",
       "  -3.116559198181693],\n",
       " [3.2862744483756146,\n",
       "  3.2958436570104985,\n",
       "  3.3004424975104207,\n",
       "  3.3573593056447724,\n",
       "  3.5149951538010926,\n",
       "  3.604046584700484,\n",
       "  3.6195913899759202,\n",
       "  3.6483553687681947,\n",
       "  4.410390255152451,\n",
       "  4.71564405320809])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = np.array(vect.get_feature_names())\n",
    "sorted_coef_index = clf.coef_[0].argsort() # ascending  [0] is just squeeze from shape (1,n)\n",
    "clf.coef_.shape, clf.coef_[0].shape, sorted(clf.coef_[0])[:10], sorted(clf.coef_[0])[-11:-1], "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest coefs:\n",
      "['mony' 'worst' 'false' 'worthless' 'horribly' 'messing' 'unsatisfied'\n",
      " 'blacklist' 'junk' 'garbage']\n",
      "\n",
      "Largest Coefs: \n",
      "['excelent' 'excelente' '4eeeks' 'exelente' 'efficient' 'excellent'\n",
      " 'loving' 'pleasantly' 'loves' 'mn8k2ll']\n"
     ]
    }
   ],
   "source": [
    "print('Smallest coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
    "print('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))\n",
    "# model.coef_[0][sorted_coef_index[0]] the smallest "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "## Term frequency–inverse document frequency (TFIDF)\n",
    "\n",
    "\n",
    "</font>\n",
    "\n",
    "TFIDF is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. Its value increases proportionally to the number of times a word appears in the document and decreases by the number of documents in the corpus that contain the word\n",
    "<div style=\"float:left;\">\n",
    "<br>\n",
    "    \n",
    "**Term frequency** $(tf(t,d))$ is measure of how frequent term t is in document d \n",
    "$$ tf(t,d) = \\frac{k}{n},$$ <br>$d$ - document,  $k$ - number of times word occurs in document $d$, $n$ - total number of words in document $d$.\n",
    "<br>\n",
    "Note: Various approaches can be used for term frequency e.g. *augmented frequency*, to prevent a bias towards longer documents (raw frequency divided by the raw frequency of the most occurring term in the document):\n",
    "\n",
    "$$ tf^{\\,A}(t,d) = 0.5+ 0.5\\cdot \\frac{tf(t,d)}{\\underset{t' \\in d}{max}(tf(t',d))} $$\n",
    "\n",
    "**Inverse document frequency** $(idf(t,D))$ is a measure of how much information the word provides.\n",
    "$$ idf(t,D) = log \\frac{N}{K},$$ <br>$D$ - all documents, $K$ - number of documents in $D$ that contain the word , $N$ - total number of documents in $D$. <br>\n",
    "</div>\n",
    "\n",
    "Note: Various approaches can be used for inverse document frequency \n",
    "\n",
    "<div style=\"float:left;\">\n",
    "<table width=\"500\">\n",
    "    <tr>\n",
    "        <th style=\"text-align:center\" bgcolor= white>Document1</th>\n",
    "        <th style=\"text-align:center\"  bgcolor= white >Document2</th></tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <table>\n",
    "                <tr>\n",
    "                    <th  bgcolor=gainsboro>Term</th>\n",
    "                    <th  bgcolor=gainsboro>Term Count</th></tr>\n",
    "                <tr><td>this</td><td>1</td></tr>\n",
    "                <tr><td>is</td><td>1</td></tr>\n",
    "                <tr><td>a</td><td>2</td></tr>\n",
    "                <tr><td>sample</td><td>1</td></tr>\n",
    "            </table>\n",
    "        </td>\n",
    "        <td>\n",
    "            <table>\n",
    "                <tr>\n",
    "                    <th bgcolor=gainsboro>Term</th>\n",
    "                    <th  bgcolor=gainsboro>Term Count</th></tr>\n",
    "                <tr><td>this</td><td>1</td></tr>\n",
    "                <tr><td>is</td><td>1</td></tr>\n",
    "                <tr><td>another</td><td>2</td></tr>\n",
    "                <tr><td>example</td><td>3</td></tr>\n",
    "            </table>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "<div/>\n",
    "\n",
    "<div style=\"float:left;\">\n",
    "<br>\n",
    "\n",
    "For <strong>\"this\"</strong>:\n",
    "$$ tf (\"this\", d_{1}) =  \\frac{1}{5} = 0.2, \\quad  tf (\"this\", d_{2}) =  \\frac{1}{7} \\approx 0.14, \\quad idf (\"this\", D) =  log \\frac{2}{2} =0; $$\n",
    "\n",
    "$$ tfidf(\"this\", d_{1}, D)  = 0.2 \\cdot 0 = 0, \\quad    tfidf(\"this\", d_{2}, D)  = 0.14 \\cdot 0 = 0 $$\n",
    "\n",
    "For <strong>\"example\"</strong>:\n",
    "$$ tf (\"example\", d_{1}) =  \\frac{0}{5} = 0 , \\quad  tf (\"example\", d_{2}) =  \\frac{3}{7} \\approx 0.43 , \\quad idf (\"example\", D) =  log \\frac{2}{1} \\approx 0.3; $$\n",
    "\n",
    "$$ tfidf(\"example\", d_{1}, D)  = 0 \\cdot 0.3 = 0, \\quad    tfidf(\"example\", d_{2}, D)  = 0.43 \\cdot 0.3 = 0.129 $$\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Sklearn tfidf\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Compute sklearn tfidf for sample with 2 documents \n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this': 4, 'is': 2, 'sample': 3, 'another': 0, 'example': 1}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.50154891, 0.70490949, 0.50154891],\n",
       "       [0.53428425, 0.80142637, 0.19007382, 0.        , 0.19007382]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(['this is a sample a', 'this is another example another example example'])\n",
    "tfidf_vectorizer= TfidfVectorizer().fit(X)\n",
    "X_vectorized= tfidf_vectorizer.transform(X)\n",
    "print (tfidf_vectorizer.vocabulary_)\n",
    "X_vectorized.toarray()\n",
    "# conclusion: sklearn uses different variant of computation tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Use sklearn tfidf for Amazon_Unlocked_Mobile documents \n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of features= 17,951\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer= TfidfVectorizer(min_df=5)#.fit(X_train) \n",
    "    # min_df - minimum document count to include the term, default is 1 \n",
    "    # you may also set max_features (Int or None) to return just limited number of top tfidf features \n",
    "X_train_vectorized = tfidf_vectorizer.fit_transform(X_train)\n",
    "print ('len of features= {:,}'.format(len(tfidf_vectorizer.get_feature_names()))) \n",
    "    # Note: min_df=5 caused 17,951  comparing to 53,216 acquired by count vectorizer\n",
    "    # Note: min_df=5 is also available in count vectorizer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01989111 0.01989111 0.01989111 ... 1.         1.         1.        ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 3624, 12532, 17320, ...,  7414,  2184,  4635])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_train_vectorized.shape # (231207, 17951) = (n_documents, n_features)\n",
    "sorted_tfidf_index = X_train_vectorized.max(axis=0).toarray()[0].argsort() \n",
    "    # max(axis=0) means max through all docs - will get the max of each word within all docs\n",
    "    # [0] - just squeezing     \n",
    "print (np.sort(X_train_vectorized.max(axis=0).toarray()[0]))\n",
    "sorted_tfidf_index # indices of the most tfidf terms \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_names  ['00' '000' '0000' ... 'тнιѕ' 'աɨtɦ' 'աօʀҡ']\n",
      "Smallest tfidf:\n",
      "['commenter' 'pthalo' 'warmness' 'storageso' 'aggregration' '1300'\n",
      " '625nits' 'a10' 'submarket' 'brawns']\n",
      "\n",
      "Largest tfidf: \n",
      "['defective' 'batteries' 'gooood' 'epic' 'luis' 'goood' 'basico'\n",
      " 'aceptable' 'problems' 'excellant']\n"
     ]
    }
   ],
   "source": [
    "feature_names = np.array(tfidf_vectorizer.get_feature_names())\n",
    "print ('feature_names ',feature_names)\n",
    "print('Smallest tfidf:\\n{}\\n'.format(feature_names[sorted_tfidf_index[:10]]))\n",
    "print('Largest tfidf: \\n{}'.format(feature_names[sorted_tfidf_index[:-11:-1]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Train model on features  extracted by tfidf vectorizer\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1:  0.9658793557580206\n",
      "AUC:  0.9821693136115232\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(max_iter=1000).fit(X_train_vectorized, y_train) # Train the model\n",
    "predictions = clf.predict(tfidf_vectorizer.transform(X_test)) \n",
    "print('f1: ', f1_score(y_test, predictions)) \n",
    "scores = clf.decision_function(tfidf_vectorizer.transform(X_test)) \n",
    "print('AUC: ', roc_auc_score(y_test, scores)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion: Perfromance is not worse but there are 3 times less amount of features used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest Coefs:\n",
      "['not' 'worst' 'useless' 'disappointed' 'terrible' 'return' 'waste' 'poor'\n",
      " 'horrible' 'doesn']\n",
      "\n",
      "Largest Coefs: \n",
      "['love' 'great' 'excellent' 'perfect' 'amazing' 'awesome' 'perfectly'\n",
      " 'easy' 'best' 'loves']\n"
     ]
    }
   ],
   "source": [
    "sorted_coef_index = clf.coef_[0].argsort()\n",
    "print('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
    "print('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### n-grams\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0]\n"
     ]
    }
   ],
   "source": [
    "# the problem is the following reviews are treated the same by current model\n",
    "targets= [\n",
    "    \"not an issue, phone is working\", \n",
    "    \"an issue, phone is not working\"\n",
    "]\n",
    "print(clf.predict(tfidf_vectorizer.transform(targets)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of features using n-grams vectorizer=50,000\n"
     ]
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(min_df=5, max_features=50000, ngram_range=(1,2)).fit(X_train) # Note: both limits are included\n",
    "X_train_vectorized = count_vectorizer.transform(X_train)\n",
    "print('len of features using n-grams vectorizer={:,}'.format(len(count_vectorizer.get_feature_names()))) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1:  0.9828450687149822\n",
      "AUC:  0.9889420473626309\n"
     ]
    }
   ],
   "source": [
    "clf= LogisticRegression(max_iter= 2000).fit(X_train_vectorized, y_train)\n",
    "predictions = clf.predict(count_vectorizer.transform(X_test)) \n",
    "print('f1: ', f1_score(y_test, predictions)) \n",
    "scores = clf.decision_function(count_vectorizer.transform(X_test)) \n",
    "print('AUC: ', roc_auc_score(y_test, scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest Coefs:\n",
      "['no good' 'worst' 'junk' 'not happy' 'not good' 'garbage' 'horrible'\n",
      " 'nit' 'great support' 'looks ok']\n",
      "\n",
      "Largest Coefs: \n",
      "['not bad' 'excelent' 'excelente' 'excellent' 'perfect' 'no problems'\n",
      " 'exelente' 'awesome' 'no issues' 'amazing']\n"
     ]
    }
   ],
   "source": [
    "feature_names = np.array(count_vectorizer.get_feature_names())\n",
    "sorted_coef_index = clf.coef_[0].argsort()\n",
    "print('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
    "print('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not an issue, phone is working', 'an issue, phone is not working']\n",
      "[1 0]\n"
     ]
    }
   ],
   "source": [
    "print (targets)\n",
    "print(clf.predict(count_vectorizer.transform(targets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "## Home Task \n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Load data \n",
    "\n",
    "</font>\n",
    "\n",
    "[Sentiment Analysis Dataset](https://www.kaggle.com/sonaam1234/sentimentdata)\n",
    "\n",
    "alternative source: \n",
    "<br>\n",
    "[rt-polaritydata](https://github.com/dennybritz/cnn-text-classification-tf/tree/master/data/rt-polaritydata)\n",
    "\n",
    "alternative source: \n",
    "<br>\n",
    "[Movie Review Data](http://www.cs.cornell.edu/people/pabo/movie-review-data)\n",
    "\n",
    "Each line in these two files corresponds to a single snippet (usually containing roughly one single sentence); all snippets are down-cased.  \n",
    "[More info about dataset](https://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.README.1.0.txt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of texts_neg = 5,331\n",
      "\n",
      " simplistic , silly and tedious . \n",
      "\n",
      " it's so laddish and juvenile , only teenage boys could possibly find it funny . \n",
      "\n",
      " exploitative and largely devoid of the depth or sophistication that would make watching such a graphic treatment of the crimes bearable . \n",
      "\n",
      " [garbus] discards the potential for pathological study , exhuming instead , the skewed melodrama of the circumstantial situation . \n",
      "\n",
      " a visually flashy but narratively opaque and emotionally vapid exercise in style and mystification . \n"
     ]
    }
   ],
   "source": [
    "fn='rt-polarity.neg'\n",
    "with open(fn, \"r\",encoding='utf-8', errors='ignore') as f: # some invalid symbols encountered \n",
    "    content = f.read()  \n",
    "texts_neg=  content.splitlines()\n",
    "print ('len of texts_neg = {:,}'.format (len(texts_neg)))\n",
    "for review in texts_neg[:5]:\n",
    "    print ( '\\n', review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of texts_pos = 5,331\n",
      "\n",
      " the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal . \n",
      "\n",
      " the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth . \n",
      "\n",
      " effective but too-tepid biopic\n",
      "\n",
      " if you sometimes like to go to the movies to have fun , wasabi is a good place to start . \n",
      "\n",
      " emerges as something rare , an issue movie that's so honest and keenly observed that it doesn't feel like one . \n"
     ]
    }
   ],
   "source": [
    "fn='rt-polarity.pos'\n",
    "\n",
    "with open(fn, \"r\",encoding='utf-8', errors='ignore') as f:\n",
    "    content = f.read()\n",
    "texts_pos=  content.splitlines()\n",
    "print ('len of texts_pos = {:,}'.format (len(texts_pos)))\n",
    "for review in texts_pos[:5]:\n",
    "    print ('\\n', review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Samples</th>\n",
       "      <th>Binary_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>simplistic , silly and tedious .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>it's so laddish and juvenile , only teenage bo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>exploitative and largely devoid of the depth o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[garbus] discards the potential for pathologic...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a visually flashy but narratively opaque and e...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10657</th>\n",
       "      <td>both exuberantly romantic and serenely melanch...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10658</th>\n",
       "      <td>mazel tov to a film about a family's joyous li...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10659</th>\n",
       "      <td>standing in the shadows of motown is the best ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10660</th>\n",
       "      <td>it's nice to see piscopo again after all these...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10661</th>\n",
       "      <td>provides a porthole into that noble , tremblin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10662 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Samples  Binary_rating\n",
       "0                      simplistic , silly and tedious .               0\n",
       "1      it's so laddish and juvenile , only teenage bo...              0\n",
       "2      exploitative and largely devoid of the depth o...              0\n",
       "3      [garbus] discards the potential for pathologic...              0\n",
       "4      a visually flashy but narratively opaque and e...              0\n",
       "...                                                  ...            ...\n",
       "10657  both exuberantly romantic and serenely melanch...              1\n",
       "10658  mazel tov to a film about a family's joyous li...              1\n",
       "10659  standing in the shadows of motown is the best ...              1\n",
       "10660  it's nice to see piscopo again after all these...              1\n",
       "10661  provides a porthole into that noble , tremblin...              1\n",
       "\n",
       "[10662 rows x 2 columns]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_df =  pd.DataFrame(\n",
    "    {\n",
    "        'Samples' : texts_neg,\n",
    "        'Binary_rating' : np.zeros(len(texts_neg), dtype=int)\n",
    "    }\n",
    ")\n",
    "\n",
    "pos_df =  pd.DataFrame(\n",
    "    {\n",
    "        'Samples' : texts_pos,\n",
    "        'Binary_rating' : np.ones(len(texts_pos), dtype=int)\n",
    "    }\n",
    ")\n",
    "\n",
    "samples_df = pd.concat([neg_df, pos_df])\n",
    "samples_df.reset_index(drop=True, inplace=True)\n",
    "samples_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('simplistic , silly and tedious . ', 0)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_df.iloc[0]['Samples'], samples_df.iloc[0]['Binary_rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_sample(sample):\n",
    "#     tokenizer = RegexpTokenizer(r'\\w+')\n",
    "#     tokens = tokenizer.tokenize(sample.lower())\n",
    "#     stop_words = set(stopwords.words('english'))  \n",
    "#     return ' '.join([x for x in tokens if x not in stop_words])\n",
    "\n",
    "# samples_df['Samples'] = samples_df['Samples'].apply(preprocess_sample)\n",
    "# samples_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(samples_df['Samples'],samples_df['Binary_rating'],random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(max_features=50000, ngram_range=(1,2)).fit(X_train)\n",
    "X_train_vectorized = count_vectorizer.transform(X_train)\n",
    "# tfidf_vectorizer= TfidfVectorizer(min_df=5).fit(X_train)\n",
    "# X_train_vectorized= tfidf_vectorizer.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1:  0.7811472415052977\n",
      "AUC:  0.8495728295614442\n"
     ]
    }
   ],
   "source": [
    "clf= LogisticRegression(max_iter= 2000).fit(X_train_vectorized, y_train)\n",
    "predictions = clf.predict(count_vectorizer.transform(X_test)) \n",
    "print('f1: ', f1_score(y_test, predictions)) \n",
    "scores = clf.decision_function(count_vectorizer.transform(X_test)) \n",
    "print('AUC: ', roc_auc_score(y_test, scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "## Learn more\n",
    "</font>\n",
    "\n",
    "sklearn.feature_extraction.text.CountVectorizer\n",
    "<br>\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "\n",
    "Bag-of-words model\n",
    "<br>\n",
    "https://en.wikipedia.org/wiki/Bag-of-words_model\n",
    "\n",
    "tf–idf\n",
    "<br>\n",
    "https://en.wikipedia.org/wiki/Tf%E2%80%93idf\n",
    "\n",
    "sklearn.feature_extraction.text.TfidfVectorizer\n",
    "<br>\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "\n",
    "Applied Text Mining in Python\n",
    "<br>\n",
    "https://www.coursera.org/learn/python-text-mining/home/welcome\n",
    "\n",
    "Natural Language Processing tutorial\n",
    "<br>\n",
    "https://pythonprogramming.net/tokenizing-words-sentences-nltk-tutorial/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font color = green >\n",
    "\n",
    "## Next lesson: topic modeling \n",
    "</font>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
